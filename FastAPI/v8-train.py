import os
import shutil
import random
from datetime import datetime
import argparse
import sys
from functools import partial

print = partial(print, flush=True)

def create_validation_split(train_dir, valid_dir, split_ratio=0.15):
    """ä»è®­ç»ƒé›†åˆ†å‡ºéªŒè¯é›†"""
    if os.path.exists(valid_dir):
        shutil.rmtree(valid_dir)
    os.makedirs(valid_dir)
    total_moved = 0
    for class_name in os.listdir(train_dir):
        if not os.path.isdir(os.path.join(train_dir, class_name)):
            continue
        train_class_dir = os.path.join(train_dir, class_name)
        valid_class_dir = os.path.join(valid_dir, class_name)
        os.makedirs(valid_class_dir)
        images = [f for f in os.listdir(train_class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
        random.shuffle(images)
        split_point = int(len(images) * split_ratio)
        for img in images[:split_point]:
            shutil.move(
                os.path.join(train_class_dir, img),
                os.path.join(valid_class_dir, img)
            )
            total_moved += 1
        print(f"ğŸ“ {class_name}: è®­ç»ƒé›† {len(images)-split_point} å¼ , éªŒè¯é›† {split_point} å¼ ")
    print(f"âœ… æ€»å…±åˆ†ç¦»äº† {total_moved} å¼ å›¾ç‰‡åˆ°éªŒè¯é›†")

def rename_and_cleanup_models(results_save_dir, final_accuracy):
    """é‡å‘½åæ¨¡å‹æ–‡ä»¶å¹¶æ¸…ç†ï¼Œåªä¿ç•™æœ€ä½³æ¨¡å‹"""
    weights_dir = os.path.join(results_save_dir, 'weights')
    if not os.path.exists(weights_dir):
        print("âŒ weightsæ–‡ä»¶å¤¹ä¸å­˜åœ¨")
        return
    accuracy_str = f"{final_accuracy:.2f}%".replace('.', '_')
    best_pt = os.path.join(weights_dir, 'best.pt')
    if not os.path.exists(best_pt):
        print("âŒ æ‰¾ä¸åˆ°best.ptæ–‡ä»¶")
        return
    new_best_name = f"best-{accuracy_str}.pt"
    new_best_path = os.path.join(weights_dir, new_best_name)
    try:
        existing_best_files = [f for f in os.listdir(weights_dir) if f.startswith('best-') and f.endswith('.pt')]
        should_update_best = True
        if existing_best_files:
            for old_best in existing_best_files:
                old_acc_str = old_best.replace('best-', '').replace('.pt', '').replace('_', '.')
                try:
                    old_acc = float(old_acc_str.replace('%', ''))
                    if final_accuracy <= old_acc:
                        print(f"â„¹ï¸  å½“å‰æ¨¡å‹å‡†ç¡®ç‡({final_accuracy:.2f}%)ä¸å¦‚ç°æœ‰bestæ¨¡å‹({old_acc:.2f}%)ï¼Œä¿ç•™ç°æœ‰bestæ¨¡å‹")
                        should_update_best = False
                        break
                    else:
                        os.remove(os.path.join(weights_dir, old_best))
                        print(f"ğŸ”„ åˆ é™¤æ—§çš„bestæ¨¡å‹: {old_best} (å‡†ç¡®ç‡: {old_acc:.2f}%)")
                except ValueError:
                    os.remove(os.path.join(weights_dir, old_best))
        if should_update_best:
            shutil.move(best_pt, new_best_path)
            print(f"âœ… ä¿ç•™æœ€ä½³æ¨¡å‹: {new_best_name}")
        else:
            os.remove(best_pt)
        for file in os.listdir(weights_dir):
            if file.endswith('.pt') and file != new_best_name:
                file_path = os.path.join(weights_dir, file)
                os.remove(file_path)
                print(f"ğŸ—‘ï¸  åˆ é™¤æ¨¡å‹æ–‡ä»¶: {file}")
        remaining_models = [f for f in os.listdir(weights_dir) if f.endswith('.pt')]
        print(f"ğŸ¯ æœ€ç»ˆä¿ç•™æ¨¡å‹: {remaining_models[0] if remaining_models else 'æ— '}")
        print(f"ğŸ“ weightsæ–‡ä»¶å¤¹ä¸­çš„æ¨¡å‹æ–‡ä»¶æ•°é‡: {len(remaining_models)}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")

# æ–°å¢ï¼šè¿‡æ‹Ÿåˆåˆ†æå‡½æ•°ï¼ˆå¯¹æ¯”è®­ç»ƒ/éªŒè¯æŸå¤±ï¼‰
def analyze_overfitting(results_save_dir):
    """
    åˆ†æè¿‡æ‹Ÿåˆé£é™©ï¼šé€šè¿‡å¯¹æ¯”è®­ç»ƒæŸå¤±ï¼ˆtrain/lossï¼‰å’ŒéªŒè¯æŸå¤±ï¼ˆval/lossï¼‰çš„å·®è·
    é€»è¾‘ï¼šå–æœ€å10è½®çš„æŸå¤±å‡å€¼ï¼Œå·®è·è¶Šå¤§ï¼Œè¿‡æ‹Ÿåˆé£é™©è¶Šé«˜
    """
    results_csv = os.path.join(results_save_dir, 'results.csv')
    if not os.path.exists(results_csv):
        print("âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒæŒ‡æ ‡æ–‡ä»¶ï¼ˆresults.csvï¼‰ï¼Œæ— æ³•åˆ†æè¿‡æ‹Ÿåˆ")
        return
    
    try:
        import pandas as pd
        df = pd.read_csv(results_csv)
        
        # å…¼å®¹ä¸åŒYOLOv8ç‰ˆæœ¬çš„åˆ—å
        train_loss_col = None
        val_loss_col = None
        if 'train/loss' in df.columns:
            train_loss_col = 'train/loss'
        elif 'metrics/train/loss' in df.columns:
            train_loss_col = 'metrics/train/loss'
        
        if 'val/loss' in df.columns:
            val_loss_col = 'val/loss'
        elif 'metrics/val/loss' in df.columns:
            val_loss_col = 'metrics/val/loss'
        
        # è‹¥åˆ—åä¸å­˜åœ¨ï¼Œæ— æ³•åˆ†æ
        if not train_loss_col or not val_loss_col:
            print(f"âš ï¸  æŒ‡æ ‡æ–‡ä»¶ä¸­ç¼ºå°‘è®­ç»ƒ/éªŒè¯æŸå¤±åˆ—ï¼Œæ— æ³•åˆ†æè¿‡æ‹Ÿåˆï¼ˆç°æœ‰åˆ—ï¼š{df.columns.tolist()}ï¼‰")
            return
        
        # è¿‡æ»¤æ‰NaNå€¼ï¼ˆè®­ç»ƒæ—©æœŸå¯èƒ½æ— éªŒè¯æŸå¤±ï¼‰
        df_valid = df.dropna(subset=[train_loss_col, val_loss_col])
        if len(df_valid) < 10:
            print(f"âš ï¸  æœ‰æ•ˆè®­ç»ƒè½®æ•°ä¸è¶³10è½®ï¼ˆä»…{len(df_valid)}è½®ï¼‰ï¼Œè¿‡æ‹Ÿåˆåˆ†æç»“æœå¯èƒ½ä¸å‡†ç¡®")
            # å–æ‰€æœ‰æœ‰æ•ˆè½®æ•°ï¼Œè€Œéå›ºå®š10è½®
            last_n = len(df_valid)
        else:
            last_n = 10  # å–æœ€å10è½®ï¼Œåæ˜ è®­ç»ƒåæœŸçš„æŸå¤±è¶‹åŠ¿
        
        # è®¡ç®—æœ€åNè½®çš„å¹³å‡æŸå¤±
        last_n_data = df_valid.tail(last_n)
        avg_train_loss = last_n_data[train_loss_col].mean()
        avg_val_loss = last_n_data[val_loss_col].mean()
        loss_gap = avg_val_loss - avg_train_loss  # éªŒè¯æŸå¤± - è®­ç»ƒæŸå¤±ï¼ˆå·®è·è¶Šå¤§è¶Šå±é™©ï¼‰
        
        # è¾“å‡ºè¿‡æ‹Ÿåˆè¯„ä¼°ç»“æœ
        print("\n" + "="*60)
        print("ğŸ“Š è¿‡æ‹Ÿåˆé£é™©è¯„ä¼°ï¼ˆåŸºäºæœ€å{}è½®æŸå¤±ï¼‰".format(last_n))
        print("-"*60)
        print(f"è®­ç»ƒæŸå¤±å‡å€¼: {avg_train_loss:.4f}")
        print(f"éªŒè¯æŸå¤±å‡å€¼: {avg_val_loss:.4f}")
        print(f"æŸå¤±å·®è·ï¼ˆéªŒè¯-è®­ç»ƒï¼‰: {loss_gap:.4f}")
        print("-"*60)
        
        # å®šä¹‰é£é™©ç­‰çº§ï¼ˆæ ¹æ®åˆ†ç±»ä»»åŠ¡å¸¸è§é˜ˆå€¼è°ƒæ•´ï¼‰
        if loss_gap < 0.1:
            print("âœ… ä½é£é™©ï¼šè®­ç»ƒ/éªŒè¯æŸå¤±æ¥è¿‘ï¼Œè¿‡æ‹Ÿåˆé£é™©æä½")
            print("   å»ºè®®ï¼šä¿æŒå½“å‰å‚æ•°ï¼Œæ— éœ€è°ƒæ•´")
        elif 0.1 <= loss_gap < 0.5:
            print("âš ï¸  ä¸­ç­‰é£é™©ï¼šéªŒè¯æŸå¤±ç•¥é«˜äºè®­ç»ƒæŸå¤±ï¼Œå­˜åœ¨è½»å¾®è¿‡æ‹Ÿåˆå€¾å‘")
            print("   å»ºè®®ï¼š1. å¢åŠ è®­ç»ƒè½®æ•°ï¼ˆè‹¥æœªåˆ°patienceä¸Šé™ï¼‰ 2. åç»­å¯å°è¯•æ•°æ®å¢å¼º")
        else:
            print("âŒ é«˜é£é™©ï¼šéªŒè¯æŸå¤±è¿œé«˜äºè®­ç»ƒæŸå¤±ï¼Œå­˜åœ¨æ˜æ˜¾è¿‡æ‹Ÿåˆ")
            print("   å»ºè®®ï¼š1. ç«‹å³åœæ­¢è®­ç»ƒï¼ˆé¿å…ç»§ç»­è¿‡æ‹Ÿåˆï¼‰ 2. å¢åŠ æ•°æ®é‡æˆ–ä½¿ç”¨æ•°æ®å¢å¼º")
            print("        3. å°è¯•å‡å°æ¨¡å‹å°ºå¯¸ï¼ˆå¦‚ä»yolov8sæ¢æˆyolov8nï¼‰æˆ–æ·»åŠ æ­£åˆ™åŒ–")
        print("="*60 + "\n")
    
    except ImportError:
        print("âš ï¸  æœªå®‰è£…pandasï¼Œæ— æ³•åˆ†æè¿‡æ‹Ÿåˆï¼ˆéœ€æ‰§è¡Œï¼špip install pandasï¼‰")
    except Exception as e:
        print(f"âš ï¸  è¿‡æ‹Ÿåˆåˆ†æå¤±è´¥ï¼š{str(e)}")

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', type=str, default=None)
    parser.add_argument('--epochs', type=int, default=None)
    parser.add_argument('--batch_size', type=int, default=None)
    parser.add_argument('--img_size', type=int, default=640)
    parser.add_argument('--model_type', type=str, default=None)
    return parser.parse_args()

def main():
    print("=== YOLOv8 é˜¿å°”èŒ¨æµ·é»˜ç—…MRIå›¾åƒåˆ†ç±»è®­ç»ƒ ===\n")
    args = parse_args()

    # 1. æ”¯æŒå‘½ä»¤è¡Œå‚æ•°è‡ªåŠ¨è®­ç»ƒ
    if args.dataset and args.epochs and args.batch_size and args.model_type:
        dataset_root = args.dataset
        epochs = args.epochs
        batch = args.batch_size
        img_size = args.img_size
        model_size = args.model_type
        if not os.path.exists(os.path.join(dataset_root, 'train')):
            print("âŒ æ‰¾ä¸åˆ° train æ–‡ä»¶å¤¹")
            return
        train_dir = os.path.join(dataset_root, 'train')
        class_names = [f for f in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, f))]
        class_names.sort()
        print(f"[è‡ªåŠ¨æ¨¡å¼] æ•°æ®é›†: {dataset_root}, è½®æ•°: {epochs}, æ‰¹æ¬¡: {batch}, å°ºå¯¸: {img_size}, æ¨¡å‹: {model_size}")
        print(f"ğŸ§  æ£€æµ‹åˆ° {len(class_names)} ä¸ªé˜¿å°”èŒ¨æµ·é»˜ç—…ç¨‹åº¦ç±»åˆ«:")
        total_images = 0
        for class_name in class_names:
            class_dir = os.path.join(train_dir, class_name)
            images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
            print(f"   ğŸ“‹ {class_name}: {len(images)} å¼ MRIå›¾åƒ")
            total_images += len(images)
        print(f"ğŸ“Š è®­ç»ƒé›†æ€»è®¡: {total_images} å¼ 640Ã—640 MRIå›¾åƒ")
        valid_dir = os.path.join(dataset_root, 'valid')
        if not os.path.exists(valid_dir):
            print("\nğŸ”„ éªŒè¯é›†ä¸å­˜åœ¨ï¼Œæ­£åœ¨ä»è®­ç»ƒé›†ä¸­åˆ†ç¦»15%ä½œä¸ºéªŒè¯é›†...")
            create_validation_split(train_dir, valid_dir)
        else:
            print("âœ… éªŒè¯é›†å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ†ç¦»æ­¥éª¤")
    else:
        if not sys.stdin.isatty():
            print("âŒ éäº¤äº’æ¨¡å¼ä¸”å¿…è¦å‚æ•°æœªæä¾›ï¼Œè®­ç»ƒå·²å–æ¶ˆï¼ˆé¿å…é˜»å¡ï¼‰ã€‚è¯·é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æˆ–APIä¼ å…¥å®Œæ•´å‚æ•°ã€‚ï¼‰")
            return

        dataset_root = input("ğŸ“‚ è¯·è¾“å…¥æ•°æ®é›†è·¯å¾„: ").strip()
        if not os.path.exists(os.path.join(dataset_root, 'train')):
            print("âŒ æ‰¾ä¸åˆ° train æ–‡ä»¶å¤¹")
            return
        train_dir = os.path.join(dataset_root, 'train')
        class_names = [f for f in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, f))]
        class_names.sort()
        print(f"ğŸ§  æ£€æµ‹åˆ° {len(class_names)} ä¸ªé˜¿å°”èŒ¨æµ·é»˜ç—…ç¨‹åº¦ç±»åˆ«:")
        total_images = 0
        for class_name in class_names:
            class_dir = os.path.join(train_dir, class_name)
            images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
            print(f"   ğŸ“‹ {class_name}: {len(images)} å¼ MRIå›¾åƒ")
            total_images += len(images)
        print(f"ğŸ“Š è®­ç»ƒé›†æ€»è®¡: {total_images} å¼ 640Ã—640 MRIå›¾åƒ")
        valid_dir = os.path.join(dataset_root, 'valid')
        if not os.path.exists(valid_dir):
            print("\nğŸ”„ éªŒè¯é›†ä¸å­˜åœ¨ï¼Œæ­£åœ¨ä»è®­ç»ƒé›†ä¸­åˆ†ç¦»15%ä½œä¸ºéªŒè¯é›†...")
            create_validation_split(train_dir, valid_dir)
        else:
            print("âœ… éªŒè¯é›†å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ†ç¦»æ­¥éª¤")
        print("\nâš™ï¸  è®­ç»ƒå‚æ•°é…ç½®:")
        model_size = input("ğŸ¤– æ¨¡å‹å¤§å° [n(æœ€å¿«)/s(æ¨è)/m/l/x(æœ€å‡†), é»˜è®¤s]: ").strip() or 's'
        epochs = int(input("ğŸ”„ è®­ç»ƒè½®æ•° [é»˜è®¤50]: ").strip() or '50')
        print("ğŸ›ï¸  æ‰¹æ¬¡å¤§å°é€‰æ‹© (æ ¹æ®æ‚¨çš„RTX 3060 6GBæ˜¾å­˜):")
        print("   8  - ä¿å®ˆæ¨¡å¼ (æ¨èï¼Œé¿å…æ˜¾å­˜ä¸è¶³)")
        print("   12 - å¹³è¡¡æ¨¡å¼")
        print("   16 - æ€§èƒ½æ¨¡å¼ (å¯èƒ½æ˜¾å­˜ä¸è¶³)")
        batch = int(input("é€‰æ‹©æ‰¹æ¬¡å¤§å° [é»˜è®¤8]: ").strip() or '8')
        img_size = 640
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒé…ç½®:")
        print(f"   ğŸ“± æ¨¡å‹: YOLOv8{model_size}-cls")
        print(f"   ğŸ”„ è½®æ•°: {epochs} è½®")
        print(f"   ğŸ“¦ æ‰¹æ¬¡: {batch} å¼ /æ‰¹")
        print(f"   ğŸ–¼ï¸  å›¾åƒ: 640Ã—640 åƒç´ ")
        print(f"   ğŸ¯ ç±»åˆ«: {len(class_names)} ä¸ªé˜¿å°”èŒ¨æµ·é»˜ç—…ç¨‹åº¦")

    try:
        from ultralytics import YOLO
        model_name = f'yolov8{model_size}-cls.pt'
        print(f"\nğŸ¤– æ­£åœ¨åŠ è½½ {model_name} åˆ†ç±»æ¨¡å‹...")
        if not os.path.exists(model_name):
            print(f"ğŸ“¥ é¦–æ¬¡ä½¿ç”¨ï¼Œæ­£åœ¨ä¸‹è½½ {model_name}...")
        model = YOLO(model_name)
        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_name}")
        print("ğŸš€ å¼€å§‹è®­ç»ƒ - YOLOv8 é˜¿å°”èŒ¨æµ·é»˜ç—…MRIå›¾åƒåˆ†ç±»")
        print("=" * 60)
        results = model.train(
            data=dataset_root,
            epochs=epochs,
            batch=batch,
            imgsz=img_size,
            project='results',
            name=f'alzheimer_v8_{model_size}_{datetime.now().strftime("%m%d_%H%M")}',
            plots=True,  # YOLOv8ä¼šè‡ªåŠ¨ç”ŸæˆæŸå¤±æ›²çº¿ï¼ˆresults.pngï¼‰
            val=True,
            patience=10,
            save_period=-1,
            workers=4,
            device=0,
            cache=False,
            amp=True,
            model=model_name,
        )
        print("=" * 60)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼é˜¿å°”èŒ¨æµ·é»˜ç—…MRIåˆ†ç±»æ¨¡å‹è®­ç»ƒæˆåŠŸï¼")
        print(f"ğŸ“ è®­ç»ƒç»“æœä¿å­˜åœ¨: {results.save_dir}")
        final_accuracy = 0
        try:
            if hasattr(results, 'results_dict'):
                final_accuracy = results.results_dict.get('metrics/accuracy_top1', 0) * 100
            elif hasattr(results, 'best_fitness'):
                final_accuracy = results.best_fitness * 100
            else:
                results_csv = os.path.join(results.save_dir, 'results.csv')
                if os.path.exists(results_csv):
                    import pandas as pd
                    df = pd.read_csv(results_csv)
                    if 'val/accuracy_top1' in df.columns:
                        final_accuracy = df['val/accuracy_top1'].max() * 100
                    elif 'metrics/accuracy_top1' in df.columns:
                        final_accuracy = df['metrics/accuracy_top1'].max() * 100
            print(f"ğŸ¯ æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {final_accuracy:.2f}%")
            rename_and_cleanup_models(results.save_dir, final_accuracy)
            
            # æ–°å¢ï¼šè°ƒç”¨è¿‡æ‹Ÿåˆåˆ†æå‡½æ•°ï¼ˆè®­ç»ƒå®Œæˆåè‡ªåŠ¨æ‰§è¡Œï¼‰
            analyze_overfitting(results.save_dir)
            
        except Exception as e:
            print(f"âš ï¸  è·å–å‡†ç¡®ç‡å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
            final_accuracy = 85.0
            rename_and_cleanup_models(results.save_dir, final_accuracy)
            # å³ä½¿å‡†ç¡®ç‡è·å–å¤±è´¥ï¼Œä¹Ÿå°è¯•åˆ†æè¿‡æ‹Ÿåˆ
            analyze_overfitting(results.save_dir)
        
        # æç¤ºæŸå¤±æ›²çº¿å›¾è¡¨ä½ç½®ï¼ˆæ–¹ä¾¿ç”¨æˆ·æŸ¥çœ‹å¯è§†åŒ–ç»“æœï¼‰
        loss_curve_path = os.path.join(results.save_dir, 'results.png')
        if os.path.exists(loss_curve_path):
            print(f"ğŸ“ˆ è®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿å·²ä¿å­˜è‡³: {loss_curve_path}")
        print(f"ğŸ”– ä½¿ç”¨çš„æ¨¡å‹: {model_name}")
    except ImportError:
        print("âŒ è¯·å…ˆå®‰è£… ultralytics: pip install ultralytics")
    except RuntimeError as e:
        if "CUDA error: out of memory" in str(e):
            print("âŒ GPUæ˜¾å­˜ä¸è¶³ï¼å»ºè®®:")
            print("   ğŸ”§ é‡æ–°è¿è¡Œï¼Œé€‰æ‹©æ‰¹æ¬¡å¤§å°ä¸º 4 æˆ– 6")
            print("   ğŸ’¾ æˆ–è€…é€‰æ‹©æ›´å°çš„æ¨¡å‹ (yolo8n)")
            print("   ğŸ–¥ï¸  æˆ–è€…åœ¨ä»£ç ä¸­æ·»åŠ  device='cpu' ä½¿ç”¨CPUè®­ç»ƒ")
        else:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")

if __name__ == "__main__":
    random.seed(42)
    main()