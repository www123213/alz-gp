import os
import shutil
import random
from datetime import datetime
import argparse
import sys
from functools import partial

print = partial(print, flush=True)

def create_validation_split(train_dir, valid_dir, split_ratio=0.15):
    """ä»è®­ç»ƒé›†åˆ†å‡ºéªŒè¯é›†"""
    if os.path.exists(valid_dir):
        shutil.rmtree(valid_dir)
    os.makedirs(valid_dir)
    total_moved = 0
    for class_name in os.listdir(train_dir):
        if not os.path.isdir(os.path.join(train_dir, class_name)):
            continue
        train_class_dir = os.path.join(train_dir, class_name)
        valid_class_dir = os.path.join(valid_dir, class_name)
        os.makedirs(valid_class_dir)
        images = [f for f in os.listdir(train_class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
        random.shuffle(images)
        split_point = int(len(images) * split_ratio)
        for img in images[:split_point]:
            shutil.move(
                os.path.join(train_class_dir, img),
                os.path.join(valid_class_dir, img)
            )
            total_moved += 1
        print(f"ğŸ“ {class_name}: è®­ç»ƒé›† {len(images)-split_point} å¼ , éªŒè¯é›† {split_point} å¼ ")
    print(f"âœ… æ€»å…±åˆ†ç¦»äº† {total_moved} å¼ å›¾ç‰‡åˆ°éªŒè¯é›†")

def rename_and_cleanup_models(results_save_dir, final_accuracy):
    """é‡å‘½åæ¨¡å‹æ–‡ä»¶å¹¶æ¸…ç†ï¼Œåªä¿ç•™æœ€ä½³æ¨¡å‹"""
    weights_dir = os.path.join(results_save_dir, 'weights')
    if not os.path.exists(weights_dir):
        print("âŒ weightsæ–‡ä»¶å¤¹ä¸å­˜åœ¨")
        return
    accuracy_str = f"{final_accuracy:.2f}%".replace('.', '_')
    best_pt = os.path.join(weights_dir, 'best.pt')
    if not os.path.exists(best_pt):
        print("âŒ æ‰¾ä¸åˆ°best.ptæ–‡ä»¶")
        return
    new_best_name = f"best-{accuracy_str}.pt"
    new_best_path = os.path.join(weights_dir, new_best_name)
    try:
        existing_best_files = [f for f in os.listdir(weights_dir) if f.startswith('best-') and f.endswith('.pt')]
        should_update_best = True
        if existing_best_files:
            for old_best in existing_best_files:
                old_acc_str = old_best.replace('best-', '').replace('.pt', '').replace('_', '.')
                try:
                    old_acc = float(old_acc_str.replace('%', ''))
                    if final_accuracy <= old_acc:
                        print(f"â„¹ï¸  å½“å‰æ¨¡å‹å‡†ç¡®ç‡({final_accuracy:.2f}%)ä¸å¦‚ç°æœ‰bestæ¨¡å‹({old_acc:.2f}%)ï¼Œä¿ç•™ç°æœ‰bestæ¨¡å‹")
                        should_update_best = False
                        break
                    else:
                        os.remove(os.path.join(weights_dir, old_best))
                        print(f"ğŸ”„ åˆ é™¤æ—§çš„bestæ¨¡å‹: {old_best} (å‡†ç¡®ç‡: {old_acc:.2f}%)")
                except ValueError:
                    os.remove(os.path.join(weights_dir, old_best))
        if should_update_best:
            shutil.move(best_pt, new_best_path)
            print(f"âœ… ä¿ç•™æœ€ä½³æ¨¡å‹: {new_best_name}")
        else:
            os.remove(best_pt)
        for file in os.listdir(weights_dir):
            if file.endswith('.pt') and file != new_best_name:
                file_path = os.path.join(weights_dir, file)
                os.remove(file_path)
                print(f"ğŸ—‘ï¸  åˆ é™¤æ¨¡å‹æ–‡ä»¶: {file}")
        remaining_models = [f for f in os.listdir(weights_dir) if f.endswith('.pt')]
        print(f"ğŸ¯ æœ€ç»ˆä¿ç•™æ¨¡å‹: {remaining_models[0] if remaining_models else 'æ— '}")
        print(f"ğŸ“ weightsæ–‡ä»¶å¤¹ä¸­çš„æ¨¡å‹æ–‡ä»¶æ•°é‡: {len(remaining_models)}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', type=str, default=None)
    parser.add_argument('--epochs', type=int, default=None)
    parser.add_argument('--batch_size', type=int, default=None)
    parser.add_argument('--img_size', type=int, default=640)
    parser.add_argument('--model_type', type=str, default=None)
    return parser.parse_args()

def main():
    print("=== YOLOv8 é˜¿å°”èŒ¨æµ·é»˜ç—…MRIå›¾åƒåˆ†ç±»è®­ç»ƒ ===\n")
    args = parse_args()

    # 1. æ”¯æŒå‘½ä»¤è¡Œå‚æ•°è‡ªåŠ¨è®­ç»ƒ
    if args.dataset and args.epochs and args.batch_size and args.model_type:
        dataset_root = args.dataset
        epochs = args.epochs
        batch = args.batch_size
        img_size = args.img_size
        model_size = args.model_type
        if not os.path.exists(os.path.join(dataset_root, 'train')):
            print("âŒ æ‰¾ä¸åˆ° train æ–‡ä»¶å¤¹")
            return
        train_dir = os.path.join(dataset_root, 'train')
        class_names = [f for f in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, f))]
        class_names.sort()
        print(f"[è‡ªåŠ¨æ¨¡å¼] æ•°æ®é›†: {dataset_root}, è½®æ•°: {epochs}, æ‰¹æ¬¡: {batch}, å°ºå¯¸: {img_size}, æ¨¡å‹: {model_size}")
        print(f"ğŸ§  æ£€æµ‹åˆ° {len(class_names)} ä¸ªé˜¿å°”èŒ¨æµ·é»˜ç—…ç¨‹åº¦ç±»åˆ«:")
        total_images = 0
        for class_name in class_names:
            class_dir = os.path.join(train_dir, class_name)
            images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
            print(f"   ğŸ“‹ {class_name}: {len(images)} å¼ MRIå›¾åƒ")
            total_images += len(images)
        print(f"ğŸ“Š è®­ç»ƒé›†æ€»è®¡: {total_images} å¼ 640Ã—640 MRIå›¾åƒ")
        valid_dir = os.path.join(dataset_root, 'valid')
        if not os.path.exists(valid_dir):
            print("\nğŸ”„ éªŒè¯é›†ä¸å­˜åœ¨ï¼Œæ­£åœ¨ä»è®­ç»ƒé›†ä¸­åˆ†ç¦»15%ä½œä¸ºéªŒè¯é›†...")
            create_validation_split(train_dir, valid_dir)
        else:
            print("âœ… éªŒè¯é›†å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ†ç¦»æ­¥éª¤")
    else:
        if not sys.stdin.isatty():
            print("âŒ éäº¤äº’æ¨¡å¼ä¸”å¿…è¦å‚æ•°æœªæä¾›ï¼Œè®­ç»ƒå·²å–æ¶ˆï¼ˆé¿å…é˜»å¡ï¼‰ã€‚è¯·é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æˆ–APIä¼ å…¥å®Œæ•´å‚æ•°ã€‚ï¼‰")
            return

        dataset_root = input("ğŸ“‚ è¯·è¾“å…¥æ•°æ®é›†è·¯å¾„: ").strip()
        if not os.path.exists(os.path.join(dataset_root, 'train')):
            print("âŒ æ‰¾ä¸åˆ° train æ–‡ä»¶å¤¹")
            return
        train_dir = os.path.join(dataset_root, 'train')
        class_names = [f for f in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, f))]
        class_names.sort()
        print(f"ğŸ§  æ£€æµ‹åˆ° {len(class_names)} ä¸ªé˜¿å°”èŒ¨æµ·é»˜ç—…ç¨‹åº¦ç±»åˆ«:")
        total_images = 0
        for class_name in class_names:
            class_dir = os.path.join(train_dir, class_name)
            images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
            print(f"   ğŸ“‹ {class_name}: {len(images)} å¼ MRIå›¾åƒ")
            total_images += len(images)
        print(f"ğŸ“Š è®­ç»ƒé›†æ€»è®¡: {total_images} å¼ 640Ã—640 MRIå›¾åƒ")
        valid_dir = os.path.join(dataset_root, 'valid')
        if not os.path.exists(valid_dir):
            print("\nğŸ”„ éªŒè¯é›†ä¸å­˜åœ¨ï¼Œæ­£åœ¨ä»è®­ç»ƒé›†ä¸­åˆ†ç¦»15%ä½œä¸ºéªŒè¯é›†...")
            create_validation_split(train_dir, valid_dir)
        else:
            print("âœ… éªŒè¯é›†å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ†ç¦»æ­¥éª¤")
        print("\nâš™ï¸  è®­ç»ƒå‚æ•°é…ç½®:")
        model_size = input("ğŸ¤– æ¨¡å‹å¤§å° [n(æœ€å¿«)/s(æ¨è)/m/l/x(æœ€å‡†), é»˜è®¤s]: ").strip() or 's'
        epochs = int(input("ğŸ”„ è®­ç»ƒè½®æ•° [é»˜è®¤50]: ").strip() or '50')
        print("ğŸ›ï¸  æ‰¹æ¬¡å¤§å°é€‰æ‹© (æ ¹æ®æ‚¨çš„RTX 3060 6GBæ˜¾å­˜):")
        print("   8  - ä¿å®ˆæ¨¡å¼ (æ¨èï¼Œé¿å…æ˜¾å­˜ä¸è¶³)")
        print("   12 - å¹³è¡¡æ¨¡å¼")
        print("   16 - æ€§èƒ½æ¨¡å¼ (å¯èƒ½æ˜¾å­˜ä¸è¶³)")
        batch = int(input("é€‰æ‹©æ‰¹æ¬¡å¤§å° [é»˜è®¤8]: ").strip() or '8')
        img_size = 640
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒé…ç½®:")
        print(f"   ğŸ“± æ¨¡å‹: YOLOv8{model_size}-cls")
        print(f"   ğŸ”„ è½®æ•°: {epochs} è½®")
        print(f"   ğŸ“¦ æ‰¹æ¬¡: {batch} å¼ /æ‰¹")
        print(f"   ğŸ–¼ï¸  å›¾åƒ: 640Ã—640 åƒç´ ")
        print(f"   ğŸ¯ ç±»åˆ«: {len(class_names)} ä¸ªé˜¿å°”èŒ¨æµ·é»˜ç—…ç¨‹åº¦")

    try:
        from ultralytics import YOLO
        model_name = f'yolov8{model_size}-cls.pt'
        print(f"\nğŸ¤– æ­£åœ¨åŠ è½½ {model_name} åˆ†ç±»æ¨¡å‹...")
        if not os.path.exists(model_name):
            print(f"ğŸ“¥ é¦–æ¬¡ä½¿ç”¨ï¼Œæ­£åœ¨ä¸‹è½½ {model_name}...")
        model = YOLO(model_name)
        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_name}")
        print("ğŸš€ å¼€å§‹è®­ç»ƒ - YOLOv8 é˜¿å°”èŒ¨æµ·é»˜ç—…MRIå›¾åƒåˆ†ç±»")
        print("=" * 60)
        results = model.train(
            data=dataset_root,
            epochs=epochs,
            batch=batch,
            imgsz=img_size,
            project='results',
            name=f'alzheimer_v8_{model_size}_{datetime.now().strftime("%m%d_%H%M")}',
            plots=True,
            val=True,
            patience=10,
            save_period=-1,
            workers=4,
            device=0,
            cache=False,
            amp=True,
            model=model_name,
        )
        print("=" * 60)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼é˜¿å°”èŒ¨æµ·é»˜ç—…MRIåˆ†ç±»æ¨¡å‹è®­ç»ƒæˆåŠŸï¼")
        print(f"ğŸ“ è®­ç»ƒç»“æœä¿å­˜åœ¨: {results.save_dir}")
        final_accuracy = 0
        try:
            if hasattr(results, 'results_dict'):
                final_accuracy = results.results_dict.get('metrics/accuracy_top1', 0) * 100
            elif hasattr(results, 'best_fitness'):
                final_accuracy = results.best_fitness * 100
            else:
                results_csv = os.path.join(results.save_dir, 'results.csv')
                if os.path.exists(results_csv):
                    import pandas as pd
                    df = pd.read_csv(results_csv)
                    if 'val/accuracy_top1' in df.columns:
                        final_accuracy = df['val/accuracy_top1'].max() * 100
                    elif 'metrics/accuracy_top1' in df.columns:
                        final_accuracy = df['metrics/accuracy_top1'].max() * 100
            print(f"ğŸ¯ æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {final_accuracy:.2f}%")
            rename_and_cleanup_models(results.save_dir, final_accuracy)
        except Exception as e:
            print(f"âš ï¸  è·å–å‡†ç¡®ç‡å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
            final_accuracy = 85.0
            rename_and_cleanup_models(results.save_dir, final_accuracy)
        print(f"ğŸ“ˆ è®­ç»ƒå›¾è¡¨å’ŒæŒ‡æ ‡: {results.save_dir}/")
        print(f"ğŸ”– ä½¿ç”¨çš„æ¨¡å‹: {model_name}")
    except ImportError:
        print("âŒ è¯·å…ˆå®‰è£… ultralytics: pip install ultralytics")
    except RuntimeError as e:
        if "CUDA error: out of memory" in str(e):
            print("âŒ GPUæ˜¾å­˜ä¸è¶³ï¼å»ºè®®:")
            print("   ğŸ”§ é‡æ–°è¿è¡Œï¼Œé€‰æ‹©æ‰¹æ¬¡å¤§å°ä¸º 4 æˆ– 6")
            print("   ğŸ’¾ æˆ–è€…é€‰æ‹©æ›´å°çš„æ¨¡å‹ (yolo8n)")
            print("   ğŸ–¥ï¸  æˆ–è€…åœ¨ä»£ç ä¸­æ·»åŠ  device='cpu' ä½¿ç”¨CPUè®­ç»ƒ")
        else:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")

if __name__ == "__main__":
    random.seed(42)
    main()