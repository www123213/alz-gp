import os
import shutil
import random
from datetime import datetime
import argparse
import sys
from functools import partial
import hashlib  # å“ˆå¸Œè®¡ç®—ä¾èµ–

print = partial(print, flush=True)

def calculate_file_hash(file_path, block_size=65536):
    """è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œå€¼"""
    hasher = hashlib.md5()
    try:
        with open(file_path, 'rb') as f:
            while chunk := f.read(block_size):
                hasher.update(chunk)
        return hasher.hexdigest()
    except Exception as e:
        print(f"âš ï¸  è®¡ç®— {file_path} å“ˆå¸Œå¤±è´¥ï¼š{str(e)}")
        return None


def remove_duplicate_exact(train_dir, backup_confirmed):
    """
    ç§»é™¤å®Œå…¨é‡å¤å›¾åƒ
    :param train_dir: è®­ç»ƒé›†ç›®å½•
    :param backup_confirmed: ç¡®è®¤çŠ¶æ€ï¼ˆTrue/Falseï¼‰
    :return: æ€»åˆ é™¤æ•°é‡
    """
    if not backup_confirmed:
        print("â„¹ï¸  æœªé€‰æ‹©æ‰§è¡Œå®Œå…¨å»é‡æ“ä½œ")
        return 0

    hash_map = {}
    total_deleted = 0

    # æŒ‰ç±»åˆ«éå†å»é‡
    for class_name in os.listdir(train_dir):
        class_path = os.path.join(train_dir, class_name)
        if not os.path.isdir(class_path):
            continue
        
        print(f"\nğŸ“‚ å¤„ç†ç±»åˆ«ï¼š{class_name}")
        class_deleted = 0

        for img_name in os.listdir(class_path):
            img_path = os.path.join(class_path, img_name)
            if not img_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):
                continue

            img_hash = calculate_file_hash(img_path)
            if img_hash is None:
                continue
            
            if img_hash in hash_map:
                os.remove(img_path)
                print(f"ğŸ—‘ï¸  åˆ é™¤é‡å¤å›¾åƒï¼š{img_path}ï¼ˆä¸ {hash_map[img_hash]} å®Œå…¨ç›¸åŒï¼‰")
                total_deleted += 1
                class_deleted += 1
            else:
                hash_map[img_hash] = img_path

        print(f"ğŸ“Š ç±»åˆ« {class_name} å»é‡å®Œæˆï¼šåˆ é™¤ {class_deleted} å¼ é‡å¤å›¾")
    
    return total_deleted


def create_validation_split(train_dir, valid_dir, split_ratio=0.15):
    """ä»è®­ç»ƒé›†åˆ†å‡ºéªŒè¯é›†"""
    if os.path.exists(valid_dir):
        shutil.rmtree(valid_dir)
    os.makedirs(valid_dir)
    total_moved = 0
    for class_name in os.listdir(train_dir):
        if not os.path.isdir(os.path.join(train_dir, class_name)):
            continue
        train_class_dir = os.path.join(train_dir, class_name)
        valid_class_dir = os.path.join(valid_dir, class_name)
        os.makedirs(valid_class_dir)
        images = [f for f in os.listdir(train_class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
        random.shuffle(images)
        split_point = int(len(images) * split_ratio)
        for img in images[:split_point]:
            shutil.move(
                os.path.join(train_class_dir, img),
                os.path.join(valid_class_dir, img)
            )
            total_moved += 1
        print(f"ğŸ“ {class_name}: è®­ç»ƒé›† {len(images)-split_point} å¼ , éªŒè¯é›† {split_point} å¼ ")
    print(f"âœ… æ€»å…±åˆ†ç¦»äº† {total_moved} å¼ å›¾åƒåˆ°éªŒè¯é›†")


def rename_and_cleanup_models(results_save_dir, final_accuracy):
    """é‡å‘½åæ¨¡å‹æ–‡ä»¶å¹¶æ¸…ç†"""
    weights_dir = os.path.join(results_save_dir, 'weights')
    if not os.path.exists(weights_dir):
        print("âŒ weightsæ–‡ä»¶å¤¹ä¸å­˜åœ¨")
        return
    accuracy_str = f"{final_accuracy:.2f}%".replace('.', '_')
    best_pt = os.path.join(weights_dir, 'best.pt')
    if not os.path.exists(best_pt):
        print("âŒ æ‰¾ä¸åˆ°best.ptæ–‡ä»¶")
        return
    new_best_name = f"best-{accuracy_str}.pt"
    new_best_path = os.path.join(weights_dir, new_best_name)
    try:
        existing_best_files = [f for f in os.listdir(weights_dir) if f.startswith('best-') and f.endswith('.pt')]
        should_update_best = True
        if existing_best_files:
            for old_best in existing_best_files:
                old_acc_str = old_best.replace('best-', '').replace('.pt', '').replace('_', '.')
                try:
                    old_acc = float(old_acc_str.replace('%', ''))
                    if final_accuracy <= old_acc:
                        print(f"â„¹ï¸  å½“å‰æ¨¡å‹å‡†ç¡®ç‡({final_accuracy:.2f}%)ä¸å¦‚ç°æœ‰bestæ¨¡å‹({old_acc:.2f}%)ï¼Œä¿ç•™ç°æœ‰bestæ¨¡å‹")
                        should_update_best = False
                        break
                    else:
                        os.remove(os.path.join(weights_dir, old_best))
                        print(f"ğŸ”„ åˆ é™¤æ—§çš„bestæ¨¡å‹: {old_best} (å‡†ç¡®ç‡: {old_acc:.2f}%)")
                except ValueError:
                    os.remove(os.path.join(weights_dir, old_best))
        if should_update_best:
            shutil.move(best_pt, new_best_path)
            print(f"âœ… ä¿ç•™æœ€ä½³æ¨¡å‹: {new_best_name}")
        else:
            os.remove(best_pt)
        for file in os.listdir(weights_dir):
            if file.endswith('.pt') and file != new_best_name:
                file_path = os.path.join(weights_dir, file)
                os.remove(file_path)
                print(f"ğŸ—‘ï¸  åˆ é™¤æ¨¡å‹æ–‡ä»¶: {file}")
        remaining_models = [f for f in os.listdir(weights_dir) if f.endswith('.pt')]
        print(f"ğŸ¯ æœ€ç»ˆä¿ç•™æ¨¡å‹: {remaining_models[0] if remaining_models else 'æ— '}")
        print(f"ğŸ“ weightsæ–‡ä»¶å¤¹ä¸­çš„æ¨¡å‹æ–‡ä»¶æ•°é‡: {len(remaining_models)}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")


def analyze_overfitting(results_save_dir):
    """è¿‡æ‹Ÿåˆåˆ†æ"""
    results_csv = os.path.join(results_save_dir, 'results.csv')
    if not os.path.exists(results_csv):
        print("âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒæŒ‡æ ‡æ–‡ä»¶ï¼ˆresults.csvï¼‰ï¼Œæ— æ³•åˆ†æè¿‡æ‹Ÿåˆ")
        return
    
    try:
        import pandas as pd
        df = pd.read_csv(results_csv)
        
        # å…¼å®¹ä¸åŒYOLOv8ç‰ˆæœ¬çš„åˆ—å
        train_loss_col = None
        val_loss_col = None
        if 'train/loss' in df.columns:
            train_loss_col = 'train/loss'
        elif 'metrics/train/loss' in df.columns:
            train_loss_col = 'metrics/train/loss'
        
        if 'val/loss' in df.columns:
            val_loss_col = 'val/loss'
        elif 'metrics/val/loss' in df.columns:
            val_loss_col = 'metrics/val/loss'
        
        if not train_loss_col or not val_loss_col:
            print(f"âš ï¸  æŒ‡æ ‡æ–‡ä»¶ä¸­ç¼ºå°‘è®­ç»ƒ/éªŒè¯æŸå¤±åˆ—ï¼Œæ— æ³•åˆ†æè¿‡æ‹Ÿåˆï¼ˆç°æœ‰åˆ—ï¼š{df.columns.tolist()}ï¼‰")
            return
        
        # è¿‡æ»¤NaNå€¼
        df_valid = df.dropna(subset=[train_loss_col, val_loss_col])
        last_n = len(df_valid) if len(df_valid) < 10 else 10
        last_n_data = df_valid.tail(last_n)
        
        avg_train_loss = last_n_data[train_loss_col].mean()
        avg_val_loss = last_n_data[val_loss_col].mean()
        loss_gap = avg_val_loss - avg_train_loss
        
        # è¾“å‡ºåˆ†æç»“æœ
        print("\n" + "="*60)
        print(f"ğŸ“Š è¿‡æ‹Ÿåˆé£é™©è¯„ä¼°ï¼ˆåŸºäºæœ€å{last_n}è½®æŸå¤±ï¼‰")
        print("-"*60)
        print(f"è®­ç»ƒæŸå¤±å‡å€¼: {avg_train_loss:.4f}")
        print(f"éªŒè¯æŸå¤±å‡å€¼: {avg_val_loss:.4f}")
        print(f"æŸå¤±å·®è·ï¼ˆéªŒè¯-è®­ç»ƒï¼‰: {loss_gap:.4f}")
        print("-"*60)
        if loss_gap < 0.1:
            print("âœ… ä½é£é™©ï¼šè®­ç»ƒ/éªŒè¯æŸå¤±æ¥è¿‘ï¼Œè¿‡æ‹Ÿåˆé£é™©æä½")
            print("   å»ºè®®ï¼šä¿æŒå½“å‰å‚æ•°ï¼Œæ— éœ€è°ƒæ•´")
        elif 0.1 <= loss_gap < 0.5:
            print("âš ï¸  ä¸­ç­‰é£é™©ï¼šéªŒè¯æŸå¤±ç•¥é«˜äºè®­ç»ƒæŸå¤±ï¼Œå­˜åœ¨è½»å¾®è¿‡æ‹Ÿåˆå€¾å‘")
            print("   å»ºè®®ï¼š1. å¢åŠ è®­ç»ƒè½®æ•°ï¼ˆè‹¥æœªåˆ°patienceä¸Šé™ï¼‰ 2. åç»­å¯å°è¯•æ•°æ®å¢å¼º")
        else:
            print("âŒ é«˜é£é™©ï¼šéªŒè¯æŸå¤±è¿œé«˜äºè®­ç»ƒæŸå¤±ï¼Œå­˜åœ¨æ˜æ˜¾è¿‡æ‹Ÿåˆ")
            print("   å»ºè®®ï¼š1. ç«‹å³åœæ­¢è®­ç»ƒï¼ˆé¿å…ç»§ç»­è¿‡æ‹Ÿåˆï¼‰ 2. å¢åŠ æ•°æ®é‡æˆ–ä½¿ç”¨æ•°æ®å¢å¼º")
            print("        3. å°è¯•å‡å°æ¨¡å‹å°ºå¯¸ï¼ˆå¦‚ä»yolov8sæ¢æˆyolov8nï¼‰æˆ–æ·»åŠ æ­£åˆ™åŒ–")
        print("="*60 + "\n")
    
    except ImportError:
        print("âš ï¸  æœªå®‰è£…pandasï¼Œæ— æ³•åˆ†æè¿‡æ‹Ÿåˆï¼ˆéœ€æ‰§è¡Œï¼špip install pandasï¼‰")
    except Exception as e:
        print(f"âš ï¸  è¿‡æ‹Ÿåˆåˆ†æå¤±è´¥ï¼š{str(e)}")


def parse_args():
    parser = argparse.ArgumentParser()
    # æ ¸å¿ƒè®­ç»ƒå‚æ•°
    parser.add_argument('--dataset', type=str, required=True, help='æ•°æ®é›†æ ¹ç›®å½•ï¼ˆå«trainæ–‡ä»¶å¤¹ï¼‰')
    parser.add_argument('--epochs', type=int, required=True, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, required=True, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--img_size', type=int, default=640, help='å›¾åƒå°ºå¯¸ï¼ˆé»˜è®¤640ï¼‰')
    parser.add_argument('--model_type', type=str, required=True, help='æ¨¡å‹ç±»å‹ï¼ˆn/s/m/l/xï¼‰')
    # å»é‡æ§åˆ¶å‚æ•°
    parser.add_argument('--deduplicate', action='store_true', help='æ˜¯å¦å¯ç”¨å®Œå…¨å»é‡')
    parser.add_argument('--backup_confirmed', action='store_true', help='ç¡®è®¤æ‰§è¡Œå»é‡')
    return parser.parse_args()


def main():
    print("=== YOLOv8 é˜¿å°”èŒ¨æµ·é»˜ç—‡MRIå›¾åƒåˆ†ç±»è®­ç»ƒ ===\n")
    args = parse_args()
    random.seed(42)

    dataset_root = args.dataset
    train_dir = os.path.join(dataset_root, 'train')
    if not os.path.exists(train_dir):
        print(f"âŒ è®­ç»ƒé›†ç›®å½•ä¸å­˜åœ¨ï¼š{train_dir}")
        return

    do_deduplicate = args.deduplicate and args.backup_confirmed
    if do_deduplicate:
        print("\nğŸ” å¯åŠ¨è®­ç»ƒé›†å®Œå…¨å»é‡ï¼ˆåŸºäºMD5å“ˆå¸Œï¼‰...")
        total_dup_deleted = remove_duplicate_exact(train_dir, True)
        print(f"âœ… å®Œå…¨å»é‡ç»“æŸï¼šå…±åˆ é™¤ {total_dup_deleted} å¼ é‡å¤å›¾åƒ")
    else:
        print("\nâ„¹ï¸ æœªå¯åŠ¨å®Œå…¨å»é‡æˆ–æœªå¤‡ä»½ï¼Œè·³è¿‡å»é‡æ­¥éª¤")

    # 3. æ‰“å°æ•°æ®é›†ä¿¡æ¯
    class_names = [f for f in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, f))]
    class_names.sort()
    print(f"\nğŸ“Š æ•°æ®é›†ä¿¡æ¯ï¼š")
    print(f"   è·¯å¾„: {dataset_root}")
    print(f"   æ¨¡å‹ç±»å‹: YOLOv8-{args.model_type}-cls")
    print(f"   è®­ç»ƒè½®æ•°: {args.epochs} è½®")
    print(f"   æ‰¹æ¬¡å¤§å°: {args.batch_size} å¼ /æ‰¹")
    print(f"   å›¾åƒå°ºå¯¸: {args.img_size}Ã—{args.img_size}")
    print(f"   ç±»åˆ«æ•°é‡: {len(class_names)} ä¸ªï¼ˆ{', '.join(class_names)}ï¼‰")
    
    # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„å›¾åƒæ•°é‡
    total_train_images = 0
    for class_name in class_names:
        class_dir = os.path.join(train_dir, class_name)
        img_count = len([f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])
        total_train_images += img_count
        print(f"   {class_name}: {img_count} å¼ MRIå›¾åƒ")
    print(f"   è®­ç»ƒé›†æ€»è®¡: {total_train_images} å¼ å›¾åƒ")

    # 4. åˆ†å‰²éªŒè¯é›†
    valid_dir = os.path.join(dataset_root, 'valid')
    if not os.path.exists(valid_dir):
        print("\nğŸ”„ éªŒè¯é›†ä¸å­˜åœ¨ï¼Œæ­£åœ¨ä»è®­ç»ƒé›†åˆ†ç¦»15%ä½œä¸ºéªŒè¯é›†...")
        create_validation_split(train_dir, valid_dir)
    else:
        print("\nâœ… éªŒè¯é›†å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ†å‰²æ­¥éª¤")

    # 5. å¯åŠ¨æ¨¡å‹è®­ç»ƒ
    try:
        from ultralytics import YOLO
        model_name = f'yolov8{args.model_type}-cls.pt'
        print(f"\nğŸ¤– æ­£åœ¨åŠ è½½æ¨¡å‹ï¼š{model_name}")
        if not os.path.exists(model_name):
            print(f"ğŸ“¥ é¦–æ¬¡ä½¿ç”¨ï¼Œæ­£åœ¨ä¸‹è½½ {model_name}...")
        model = YOLO(model_name)
        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_name}")

        # è®­ç»ƒç»“æœä¿å­˜è·¯å¾„
        results_name = f'alzheimer_cls_v8_{args.model_type}_{datetime.now().strftime("%m%d_%H%M")}'
        print("\nğŸš€ å¼€å§‹è®­ç»ƒ - YOLOv8 é˜¿å°”èŒ¨æµ·é»˜ç—‡MRIå›¾åƒåˆ†ç±»")
        print("=" * 60)

        results = model.train(
            data=dataset_root,
            epochs=args.epochs,
            batch=args.batch_size,
            imgsz=args.img_size,
            project='results',
            name=results_name,
            plots=True,  # YOLOv8ä¼šè‡ªåŠ¨ç”ŸæˆæŸå¤±æ›²çº¿ï¼ˆresults.pngï¼‰
            val=True,
            patience=10,  # 10è½®æ— æ”¹å–„å°±åœæ­¢
            save_period=-1,  # ç¦ç”¨å®šæœŸä¿å­˜ï¼Œåªä¿å­˜bestå’Œlast
            workers=4,  # å‡å°‘workeræ•°é‡é¿å…å†…å­˜é—®é¢˜
            device=0,  # å¼ºåˆ¶ä½¿ç”¨GPU 0
            cache=False,  # ä¸ç¼“å­˜å›¾åƒåˆ°å†…å­˜
            amp=True,  # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒèŠ‚çœæ˜¾å­˜
            model=model_name,  # æ˜ç¡®æŒ‡å®šæ¨¡å‹
            #è®­ç»ƒå¢å¼º
            augment=True,
            fliplr=0.6,    # æ°´å¹³ç¿»è½¬æ¦‚ç‡
            flipud=0.0,    # å‚ç›´ç¿»è½¬æ¦‚ç‡ï¼ˆMRIä¸€èˆ¬ä¸å»ºè®®å‚ç›´ç¿»è½¬ï¼Œè®¾ä¸º0ï¼‰
            hsv_h=0.015,   # è‰²è°ƒæ‰°åŠ¨ï¼ˆé€‚åˆç°åº¦å›¾çš„ç»†å¾®è°ƒæ•´ï¼‰
            hsv_s=0.75,     # é¥±å’Œåº¦æ‰°åŠ¨ï¼ˆå¢å¼ºå¯¹æ¯”åº¦å·®å¼‚ï¼‰
            hsv_v=0.45,     # äº®åº¦æ‰°åŠ¨ï¼ˆçªå‡ºè„‘ç»“æ„ç»†èŠ‚ï¼‰
            degrees=3.0,  # éšæœºæ—‹è½¬è§’åº¦
            translate=0.12, # å¹³ç§»æ‰°åŠ¨
            scale=0.15,     # ç¼©æ”¾æ‰°åŠ¨
            shear=0.0,    # å‰ªåˆ‡å˜æ¢
            perspective=0.0,  # é€è§†å˜æ¢ï¼ˆæ¨¡æ‹Ÿæ‰«æè§’åº¦å·®å¼‚ï¼‰
            weight_decay=0.0005, # æƒé‡è¡°å‡ï¼ˆL2æ­£åˆ™åŒ–ï¼‰
        )

        print("=" * 60)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼é˜¿å°”èŒ¨æµ·é»˜ç—‡MRIåˆ†ç±»æ¨¡å‹è®­ç»ƒæˆåŠŸï¼")
        print(f"ğŸ“ è®­ç»ƒç»“æœä¿å­˜è·¯å¾„: {results.save_dir}")

        # 6. å¤„ç†è®­ç»ƒç»“æœ
        final_accuracy = 0
        try:
            if hasattr(results, 'results_dict'):
                final_accuracy = results.results_dict.get('metrics/accuracy_top1', 0) * 100
            elif hasattr(results, 'best_fitness'):
                final_accuracy = results.best_fitness * 100
            else:
                results_csv = os.path.join(results.save_dir, 'results.csv')
                if os.path.exists(results_csv):
                    import pandas as pd
                    df = pd.read_csv(results_csv)
                    if 'val/accuracy_top1' in df.columns:
                        final_accuracy = df['val/accuracy_top1'].max() * 100
                    elif 'metrics/accuracy_top1' in df.columns:
                        final_accuracy = df['metrics/accuracy_top1'].max() * 100
            print(f"ğŸ¯ æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {final_accuracy:.2f}%")
            rename_and_cleanup_models(results.save_dir, final_accuracy)
            analyze_overfitting(results.save_dir)
            
        except Exception as e:
            print(f"âš ï¸  è·å–å‡†ç¡®ç‡å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
            final_accuracy = 85.0
            rename_and_cleanup_models(results.save_dir, final_accuracy)
            analyze_overfitting(results.save_dir)
        
        # æç¤ºæŸå¤±æ›²çº¿ä½ç½®
        loss_curve_path = os.path.join(results.save_dir, 'results.png')
        if os.path.exists(loss_curve_path):
            print(f"ğŸ“ˆ è®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿å·²ä¿å­˜è‡³: {loss_curve_path}")
        print(f"ğŸ”– ä½¿ç”¨çš„æ¨¡å‹: {model_name}")

    except ImportError:
        print("âŒ è¯·å…ˆå®‰è£… ultralytics: pip install ultralytics")
    except RuntimeError as e:
        if "CUDA error: out of memory" in str(e):
            print("âŒ GPUæ˜¾å­˜ä¸è¶³ï¼å»ºè®®:")
            print("   ğŸ”§ é‡æ–°è¿è¡Œï¼Œé€‰æ‹©æ‰¹æ¬¡å¤§å°ä¸º 4 æˆ– 6")
            print("   ğŸ’¾ æˆ–è€…é€‰æ‹©æ›´å°çš„æ¨¡å‹ (yolo8n)")
            print("   ğŸ–¥ï¸  æˆ–è€…åœ¨ä»£ç ä¸­æ·»åŠ  device='cpu' ä½¿ç”¨CPUè®­ç»ƒ")
        else:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")


if __name__ == "__main__":
    main()